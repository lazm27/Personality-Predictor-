{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df=pd.read_csv(\"changed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>moments sportscenter top ten plays pranks lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>finding lack posts alarming sex boring positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good one course say know blessing curse absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  enjoyed conversation day esoteric gabbin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>fired another silly misconception approaching ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ   moments sportscenter top ten plays pranks lif...\n",
       "1  ENTP  finding lack posts alarming sex boring positio...\n",
       "2  INTP  good one course say know blessing curse absolu...\n",
       "3  INTJ  dear  enjoyed conversation day esoteric gabbin...\n",
       "4  ENTJ  fired another silly misconception approaching ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>moments sportscenter top ten plays pranks lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>finding lack posts alarming sex boring positio...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good one course say know blessing curse absolu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear  enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>fired another silly misconception approaching ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  IE  NS  TF  JP\n",
       "0  INFJ   moments sportscenter top ten plays pranks lif...   1   1   0   1\n",
       "1  ENTP  finding lack posts alarming sex boring positio...   0   1   1   0\n",
       "2  INTP  good one course say know blessing curse absolu...   1   1   1   0\n",
       "3  INTJ  dear  enjoyed conversation day esoteric gabbin...   1   1   1   1\n",
       "4  ENTJ  fired another silly misconception approaching ...   0   1   1   1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0; N = 0\n",
    "    T = 0; J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('I-E not found') \n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('N-S not found')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('T-F not found')\n",
    "        \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('J-P not found')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
    "\n",
    "df = df.join(df.apply (lambda row: get_types (row),axis=1))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the MBTI personality into 4 letters and binarizing it\n",
    "import numpy as np\n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "#To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "#list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "#print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example :\n",
      "\n",
      "MBTI before preprocessing:\n",
      "\n",
      " INFJ\n",
      "\n",
      "MBTI after preprocessing:\n",
      "\n",
      " [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "  list_personality = []\n",
    "  list_posts = []\n",
    "  len_data = len(data)\n",
    "  i=0\n",
    "  \n",
    "  for row in data.iterrows():\n",
    "      # check code working \n",
    "      # i+=1\n",
    "      # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "      #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "      #Remove and clean comments\n",
    "      posts = row[1].posts\n",
    "\n",
    "      #Remove url links \n",
    "      #temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "      #Remove Non-words - keep only words\n",
    "      #temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "      # Remove spaces > 1\n",
    "      temp = re.sub(' +', ' ', posts).lower()\n",
    "      temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "      #Remove multiple letter repeating words\n",
    "      #temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n",
    "\n",
    "      #Remove stop words\n",
    "      #if remove_stop_words:\n",
    "      #    temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n",
    "      #else:\n",
    "      #    temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "          \n",
    "      #Remove MBTI personality words from posts\n",
    "      #if remove_mbti_profiles:\n",
    "      #    for t in unique_type_list:\n",
    "      #        temp = temp.replace(t,\"\")\n",
    "\n",
    "      # transform mbti to binary vector\n",
    "      type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n",
    "      list_personality.append(type_labelized)\n",
    "      # the cleaned data temp is passed here\n",
    "      list_posts.append(temp)\n",
    "\n",
    "  # returns the result\n",
    "  list_posts = np.array(list_posts)\n",
    "  list_personality = np.array(list_personality)\n",
    "  return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality  = pre_process_text(df, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "print(\"Example :\")\n",
    "#print(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\n",
    "#print(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\n",
    "print(\"\\nMBTI before preprocessing:\\n\\n\", df.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "  (0, 712)\t1\n",
      "  (0, 211)\t2\n",
      "  (0, 707)\t2\n",
      "  (0, 426)\t1\n",
      "  (0, 510)\t1\n",
      "  (0, 370)\t1\n",
      "  (0, 254)\t1\n",
      "  (0, 534)\t1\n",
      "  (0, 470)\t1\n",
      "  (0, 148)\t1\n",
      "  (0, 579)\t1\n",
      "  (0, 639)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 460)\t1\n",
      "  (0, 571)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 446)\t1\n",
      "  (0, 724)\t2\n",
      "  (0, 239)\t1\n",
      "  (0, 298)\t1\n",
      "  (0, 705)\t2\n",
      "  (0, 764)\t2\n",
      "  (0, 666)\t1\n",
      "  (0, 264)\t2\n",
      "  (0, 605)\t1\n",
      "  :\t:\n",
      "  (8674, 149)\t1\n",
      "  (8674, 650)\t1\n",
      "  (8674, 659)\t1\n",
      "  (8674, 752)\t1\n",
      "  (8674, 486)\t2\n",
      "  (8674, 224)\t2\n",
      "  (8674, 479)\t1\n",
      "  (8674, 68)\t2\n",
      "  (8674, 369)\t1\n",
      "  (8674, 206)\t1\n",
      "  (8674, 363)\t1\n",
      "  (8674, 575)\t3\n",
      "  (8674, 61)\t1\n",
      "  (8674, 718)\t1\n",
      "  (8674, 671)\t2\n",
      "  (8674, 757)\t2\n",
      "  (8674, 95)\t1\n",
      "  (8674, 466)\t1\n",
      "  (8674, 216)\t1\n",
      "  (8674, 376)\t1\n",
      "  (8674, 644)\t1\n",
      "  (8674, 638)\t1\n",
      "  (8674, 596)\t1\n",
      "  (8674, 517)\t1\n",
      "  (8674, 293)\t1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1000,  \n",
    "                             max_df=0.7,\n",
    "                             min_df=0.1) \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "print(X_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 feature names can be seen below\n",
      "[(0, 'ability'), (1, 'able'), (2, 'absolutely'), (3, 'accept'), (4, 'accurate'), (5, 'across'), (6, 'act'), (7, 'action'), (8, 'actual'), (9, 'actually')]\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(enumerate(cntizer.get_feature_names_out()))\n",
    "print(\"10 feature names can be seen below\")\n",
    "print(feature_names[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n",
      "(8675, 805)\n"
     ]
    }
   ],
   "source": [
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.04927502 0.         ... 0.         0.         0.        ]\n",
      " [0.15748878 0.05462647 0.13163268 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.06630823 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.04007086 0.         ... 0.         0.0539018  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "JP: Judging (J) / Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "personality_type = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n",
    "                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\"  ]\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    print(personality_type[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E) Accuracy: 78.52%\n",
      "NS: Intuition (N) / Sensing (S) Accuracy: 86.03%\n",
      "FT: Feeling (F) / Thinking (T) Accuracy: 76.35%\n",
      "JP: Judging (J) / Perceiving (P) Accuracy: 66.82%\n",
      "Trained models saved as:\n",
      "IE: Introversion (I) / Extroversion (E): model_0.joblib\n",
      "NS: Intuition (N) / Sensing (S): model_1.joblib\n",
      "FT: Feeling (F) / Thinking (T): model_2.joblib\n",
      "JP: Judging (J) / Perceiving (P): model_3.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "# Define a dictionary to store trained models\n",
    "trained_models = {}\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    Y = list_personality[:, l]\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # Fit model on training data\n",
    "    model = SVC(random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    model_filename = f\"model_{l}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    trained_models[personality_type[l]] = model_filename\n",
    "\n",
    "    # Make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))\n",
    "\n",
    "# Print the filenames of the saved models\n",
    "print(\"Trained models saved as:\")\n",
    "for personality, model_filename in trained_models.items():\n",
    "    print(f\"{personality}: {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_transformer.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the CountVectorizer\n",
    "joblib.dump(cntizer, 'count_vectorizer.pkl')\n",
    "\n",
    "# Save the TfidfTransformer\n",
    "joblib.dump(tfizer, 'tfidf_transformer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lazee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lazee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "#df=pd.read_csv('mbti_1.csv')\n",
    "\n",
    "def preprocess_data(post):\n",
    "    #data_length=[]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    #cleaned_text=[]\n",
    "    stop_words = set(stopwords.words('english')) # Load stop words\n",
    "    pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
    "    pers_types = [p.lower() for p in pers_types]\n",
    "    sentence= post\n",
    "    print(\"Cleaning The Dataset\")\n",
    "    #for sentence in tqdm(df.posts):\n",
    "\n",
    "    sentence=sentence.lower()\n",
    "\n",
    "    sentence=re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n",
    "\n",
    "    sentence=re.sub('[^0-9a-z]',' ',sentence)\n",
    "\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stop_words]) # Remove stop words\n",
    "        #print(len(sentence))\n",
    "\n",
    "    for p in pers_types:\n",
    "        sentence = re.sub(p, '', sentence)\n",
    "        #print(len(sentence))\n",
    "\n",
    "    sentence = lemmatizer.lemmatize(sentence) # Lemmatize words\n",
    "    #data_length.append(len(sentence.split())) #Split data, measure length of new filtered data\n",
    "\n",
    "    cleaned_text= sentence\n",
    "    return pd.DataFrame(data={'type': ['INFJ'], 'posts': [cleaned_text]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "  list_personality = []\n",
    "  list_posts = []\n",
    "  len_data = len(data)\n",
    "  i=0\n",
    "  \n",
    "  for row in data.iterrows():\n",
    "      # check code working \n",
    "      # i+=1\n",
    "      # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "      #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "      #Remove and clean comments\n",
    "      posts = row[1].posts\n",
    "\n",
    "      #Remove url links \n",
    "      #temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "      #Remove Non-words - keep only words\n",
    "      #temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "      # Remove spaces > 1\n",
    "      temp = re.sub(' +', ' ', posts).lower()\n",
    "      temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "      #Remove multiple letter repeating words\n",
    "      #temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n",
    "\n",
    "      #Remove stop words\n",
    "      #if remove_stop_words:\n",
    "      #    temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n",
    "      #else:\n",
    "      #    temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "          \n",
    "      #Remove MBTI personality words from posts\n",
    "      #if remove_mbti_profiles:\n",
    "      #    for t in unique_type_list:\n",
    "      #        temp = temp.replace(t,\"\")\n",
    "\n",
    "      # transform mbti to binary vector\n",
    "      type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n",
    "      list_personality.append(type_labelized)\n",
    "      # the cleaned data temp is passed here\n",
    "      list_posts.append(temp)\n",
    "\n",
    "  # returns the result\n",
    "  list_posts = np.array(list_posts)\n",
    "  list_personality = np.array(list_personality)\n",
    "  return list_posts, list_personality\n",
    "\n",
    "#list_posts, list_personality  = pre_process_text(df, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning The Dataset\n"
     ]
    }
   ],
   "source": [
    "my_posts = \"\"\" They act like they care They tell me to share But when I carve the stories on my arm The doctor just calls it self harm I’m not asking for attention There’s a reason I have apprehensions I just need you to see What has become of me||| I know I’m going crazy But they think my thoughts are just hazy When in that chaos, in that confusion I’m crying out for help, to escape my delusions||| Mental health is a state of mind How does one keep that up when assistance is denied All my failed attempts to fight the blaze You treat it like its a passing phase||| Well stop, its not, because mental illness is real Understand that we’re all not made of steel Because when you brush these issues under the carpet You make it seem like its our mistake we’re not guarded||| Don’t you realise that its a problem that needs to be addressed Starting at home, in our nest Why do you keep your mouths shut about such things Instead of caring for those with broken wings||| What use is this social stigma When mental illness is not even such an enigma Look around and you’ll see the numbers of the affected hiding under the covers ||| This is an issue that needs to be discussed Not looked down upon with disgust Mental illness needs to be accepted So that people can be protected ||| Let me give you some direction People need affection The darkness must be escaped Only then the lost can be saved||| Bring in a change Something not very strange The new year is here Its time to eradicate fear||| Recognise the wrists under the knives To stop mental illness from taking more lives Let’s break the convention Start ‘suicide prevention’.||| Hoping the festival of lights drives the darkness of mental illness away\"\"\"\n",
    "\n",
    "# The type is just a dummy so that the data prep function can be reused\n",
    "#mydata = pd.DataFrame(data={'type': ['INFJ'], 'posts': [my_posts]})\n",
    "cleaned_text=preprocess_data(my_posts)\n",
    "\n",
    "my_posts, dummy  = pre_process_text(cleaned_text, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "cntizer = joblib.load('count_vectorizer.pkl')\n",
    "\n",
    "tfizer = joblib.load('tfidf_transformer.pkl')\n",
    "\n",
    "my_X_cnt = cntizer.transform(my_posts)\n",
    "my_X_tfidf =  tfizer.transform(my_X_cnt).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E) classifier trained\n",
      "NS: Intuition (N) / Sensing (S) classifier trained\n",
      "FT: Feeling (F) / Thinking (T) classifier trained\n",
      "JP: Judging (J) / Perceiving (P) classifier trained\n"
     ]
    }
   ],
   "source": [
    "# setup parameters for xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "param = {}\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['nthread'] = 8\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "#XGBoost model for MBTI dataset\n",
    "result = []\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    print(\"%s classifier trained\" % (personality_type[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for my  data\n",
    "    y_pred = model.predict(my_X_tfidf)\n",
    "    result.append(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTP\n"
     ]
    }
   ],
   "source": [
    "print(translate_back(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SVM model\n",
    "svm_model_IE = joblib.load('model_0.joblib')\n",
    "svm_model_NS = joblib.load('model_1.joblib')\n",
    "svm_model_FT = joblib.load('model_2.joblib')\n",
    "svm_model_JP = joblib.load('model_3.joblib')\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "y_pred_IE = svm_model_IE.predict(my_X_tfidf)[0]\n",
    "y_pred_NS = svm_model_NS.predict(my_X_tfidf)[0]\n",
    "y_pred_FT = svm_model_FT.predict(my_X_tfidf)[0]\n",
    "y_pred_JP = svm_model_JP.predict(my_X_tfidf)[0]\n",
    "\n",
    "results=[y_pred_IE,y_pred_NS,y_pred_FT,y_pred_JP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTP\n"
     ]
    }
   ],
   "source": [
    "print(translate_back(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
